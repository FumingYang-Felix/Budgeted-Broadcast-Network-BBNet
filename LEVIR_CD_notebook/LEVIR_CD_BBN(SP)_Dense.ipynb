{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "abaM5IFfVquQ",
      "metadata": {
        "id": "abaM5IFfVquQ"
      },
      "source": [
        "# LEVIR-CD | Dense vs SP-in (30 epochs each)\n",
        "\n",
        "This notebook trains **two** change-detection models (Dense baseline and **SP-in**) on a LEVIR-CD–style folder:\n",
        "\n",
        "```\n",
        "root/\n",
        "  A/      # t1 images\n",
        "  B/      # t2 images\n",
        "  label/  # binary masks (0/255 or 0/1)\n",
        "```\n",
        "or nested as `root/train/{A,B,label}`, `root/val/{...}`, `root/test/{...}`.\n",
        "\n",
        "It will automatically mount Google Drive on Colab, look for a likely dataset path, and save outputs (checkpoints, metrics, overlays) to your Drive at `MyDrive/LEVIR_CD_runs/run_YYYYmmdd_HHMMSS/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bso2igY2VquT",
      "metadata": {
        "id": "bso2igY2VquT"
      },
      "outputs": [],
      "source": [
        "import os, math, json, time, argparse, sys\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def in_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def colab_mount_and_resolve_paths(user_root: Optional[str]) -> Tuple[Optional[str], str]:\n",
        "    out_dir = \"/content\"\n",
        "    if not in_colab():\n",
        "        return user_root, out_dir\n",
        "    try:\n",
        "        from google.colab import drive  # type: ignore\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "    candidates: List[str] = []\n",
        "    if user_root:\n",
        "        candidates.append(user_root)\n",
        "    candidates.extend([\n",
        "        \"/content/drive/MyDrive/LEVIR-CD\",\n",
        "        \"/content/drive/MyDrive/datasets/LEVIR-CD\",\n",
        "        \"/content/LEVIR-CD\",\n",
        "    ])\n",
        "    resolved_root: Optional[str] = None\n",
        "    for p in candidates:\n",
        "        if p and os.path.isdir(p):\n",
        "            if os.path.isdir(os.path.join(p, \"A\")) or os.path.isdir(os.path.join(p, \"train\", \"A\")):\n",
        "                resolved_root = p\n",
        "                break\n",
        "    default_drive_out = \"/content/drive/MyDrive/LEVIR_CD_runs\"\n",
        "    out_dir = default_drive_out if os.path.isdir(\"/content/drive/MyDrive\") else \"/content/outputs\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if resolved_root is None:\n",
        "        print(\"[Colab] Could not auto-locate LEVIR-CD. Please set ROOT manually in the next cell.\")\n",
        "    return resolved_root, out_dir\n",
        "\n",
        "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zbPzXPWwVquU",
      "metadata": {
        "id": "zbPzXPWwVquU"
      },
      "source": [
        "## 1) Configure paths & hyperparameters\n",
        "- If `ROOT` is None, the cell will try to auto-locate under your Drive.\n",
        "- Change `ROOT` to your dataset folder if needed.\n",
        "- `EPOCHS_DENSE` and `EPOCHS_SP` are both 30 by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iai3NnNxVquU",
      "metadata": {
        "id": "iai3NnNxVquU"
      },
      "outputs": [],
      "source": [
        "ROOT = \"/content/drive/MyDrive/LEVIR-CD\"\n",
        "RESIZE = 256\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-3\n",
        "EPOCHS_DENSE = 30\n",
        "EPOCHS_SP = 30\n",
        "SEED = 42\n",
        "\n",
        "set_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "resolved_root, default_out_dir = colab_mount_and_resolve_paths(ROOT)\n",
        "ROOT = resolved_root or ROOT\n",
        "OUT_BASE = default_out_dir\n",
        "RUN_DIR = os.path.join(OUT_BASE, time.strftime(\"run_%Y%m%d_%H%M%S\"))\n",
        "os.makedirs(RUN_DIR, exist_ok=True)\n",
        "print(\"ROOT:\", ROOT)\n",
        "print(\"RUN_DIR:\", RUN_DIR)\n",
        "assert ROOT is not None, \"Please set ROOT to the folder containing A/B/label (or nested train/val/test).\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "awZPhzAwVquV",
      "metadata": {
        "id": "awZPhzAwVquV"
      },
      "source": [
        "## 2) Dataset (LEVIR-CD style)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Ta7L3MJVquV",
      "metadata": {
        "id": "6Ta7L3MJVquV"
      },
      "outputs": [],
      "source": [
        "class LevirCD(Dataset):\n",
        "    def __init__(self, root: str, split: str = \"train\", resize: int = 256, aug: bool = True):\n",
        "        self.root = root\n",
        "        self.resize = resize\n",
        "        self.aug = aug and (split == \"train\")\n",
        "        cand = os.path.join(root, split)\n",
        "        if os.path.isdir(cand) and os.path.isdir(os.path.join(cand, \"A\")):\n",
        "            base = cand\n",
        "        else:\n",
        "            base = root\n",
        "        a_dir = os.path.join(base, \"A\")\n",
        "        b_dir = os.path.join(base, \"B\")\n",
        "        l_dir = os.path.join(base, \"label\")\n",
        "        names = set(os.listdir(a_dir)) & set(os.listdir(b_dir)) & set(os.listdir(l_dir))\n",
        "        names = sorted([n for n in names if n.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".tif\", \".bmp\"))])\n",
        "        n = len(names)\n",
        "        n_train = int(0.8 * n)\n",
        "        n_val = int(0.1 * n)\n",
        "        if split == \"train\":\n",
        "            self.names = names[:n_train]\n",
        "        elif split == \"val\":\n",
        "            self.names = names[n_train:n_train+n_val]\n",
        "        else:\n",
        "            self.names = names[n_train+n_val:]\n",
        "        self.a_dir, self.b_dir, self.l_dir = a_dir, b_dir, l_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def _load_img(self, path: str) -> np.ndarray:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.resize is not None:\n",
        "            img = img.resize((self.resize, self.resize), Image.BILINEAR)\n",
        "        return np.array(img, dtype=np.uint8)\n",
        "\n",
        "    def _load_mask(self, path: str) -> np.ndarray:\n",
        "        m = Image.open(path).convert(\"L\")\n",
        "        if self.resize is not None:\n",
        "            m = m.resize((self.resize, self.resize), Image.NEAREST)\n",
        "        arr = np.array(m, dtype=np.uint8)\n",
        "        if arr.max() > 1:\n",
        "            arr = (arr > 127).astype(np.uint8)\n",
        "        return arr\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.names[idx]\n",
        "        a = self._load_img(os.path.join(self.a_dir, name))\n",
        "        b = self._load_img(os.path.join(self.b_dir, name))\n",
        "        y = self._load_mask(os.path.join(self.l_dir, name))\n",
        "        if self.aug:\n",
        "            if np.random.rand() < 0.5:\n",
        "                a = np.flip(a, axis=1).copy(); b = np.flip(b, axis=1).copy(); y = np.flip(y, axis=1).copy()\n",
        "            if np.random.rand() < 0.5:\n",
        "                a = np.flip(a, axis=0).copy(); b = np.flip(b, axis=0).copy(); y = np.flip(y, axis=0).copy()\n",
        "        a = torch.from_numpy(a).permute(2,0,1).float() / 255.0\n",
        "        b = torch.from_numpy(b).permute(2,0,1).float() / 255.0\n",
        "        y = torch.from_numpy(y).float().unsqueeze(0)\n",
        "        return a, b, y, name\n",
        "\n",
        "train_set = LevirCD(ROOT, split=\"train\", resize=RESIZE, aug=True)\n",
        "val_set   = LevirCD(ROOT, split=\"val\",   resize=RESIZE, aug=False)\n",
        "test_set  = LevirCD(ROOT, split=\"test\",  resize=RESIZE, aug=False)\n",
        "print(\"Train/Val/Test sizes:\", len(train_set), len(val_set), len(test_set))\n",
        "\n",
        "workers = int(os.getenv(\"CD_WORKERS\", \"0\"))\n",
        "pin_mem = os.getenv(\"CD_PIN_MEMORY\", \"0\") == \"1\"\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,  num_workers=workers, pin_memory=pin_mem)\n",
        "val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False, num_workers=workers, pin_memory=pin_mem)\n",
        "test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False, num_workers=workers, pin_memory=pin_mem)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q-mQp4lbVquV",
      "metadata": {
        "id": "q-mQp4lbVquV"
      },
      "source": [
        "## 3) SP-in Conv + FC-Siam-conc model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LpcKNmS1VquW",
      "metadata": {
        "id": "LpcKNmS1VquW"
      },
      "outputs": [],
      "source": [
        "class SPInConv2d(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1, bias=True,\n",
        "                 alpha=0.05, update_interval=50, warmup_steps=1000,\n",
        "                 tau_high=0.7, tau_low=0.1, dens_freq=0.4, dens_rare=0.9,\n",
        "                 min_fan=4, rescale_mode='sqrt', use_bn=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, k, s, p, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_ch) if use_bn else None\n",
        "        self.alpha = float(alpha)\n",
        "        self.update_interval = int(update_interval)\n",
        "        self.warmup_steps = int(warmup_steps)\n",
        "        self.tau_h = float(tau_high)\n",
        "        self.tau_l = float(tau_low)\n",
        "        self.d_f = float(dens_freq)\n",
        "        self.d_r = float(dens_rare)\n",
        "        self.min_fan = int(min_fan)\n",
        "        self.rescale_mode = rescale_mode\n",
        "        self.register_buffer(\"mask\", torch.ones(out_ch, in_ch, 1, 1))\n",
        "        self.register_buffer(\"activation_counts\", torch.zeros(out_ch))\n",
        "        self.register_buffer(\"step\", torch.zeros((), dtype=torch.long), persistent=False)\n",
        "        self._counter = 0\n",
        "\n",
        "    def _rescale(self, pre):\n",
        "        if self.rescale_mode == \"none\":\n",
        "            return pre\n",
        "        fan = self.mask.view(self.mask.size(0), -1).sum(dim=1).clamp_min(1.0)\n",
        "        base = float(self.mask.size(1))\n",
        "        if self.rescale_mode == \"linear\":\n",
        "            s = base / fan\n",
        "        else:\n",
        "            s = (base / fan).sqrt()\n",
        "        return pre * s.view(1, -1, 1, 1).to(pre.device, pre.dtype)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _update_mask(self):\n",
        "        eps = 1e-12\n",
        "        avg = self.activation_counts.detach().clone()\n",
        "        cur = self.mask.detach().clone()\n",
        "        W = self.conv.weight.detach().abs()\n",
        "        W2 = W.view(W.size(0), W.size(1), -1).mean(dim=2)\n",
        "        freq = (avg > self.tau_h).nonzero(as_tuple=True)[0].tolist()\n",
        "        rare = (avg < self.tau_l).nonzero(as_tuple=True)[0].tolist()\n",
        "        out_ch, in_ch = cur.size(0), cur.size(1)\n",
        "        def apply_row(i, density):\n",
        "            alive = cur[i,:,0,0].bool()\n",
        "            cand = W2[i][alive]\n",
        "            k = max(self.min_fan, int(round(in_ch * float(density))))\n",
        "            k = min(k, int(alive.sum().item()))\n",
        "            if cand.numel() <= k:\n",
        "                return\n",
        "            thr = cand.topk(k, largest=True).values[-1] + eps\n",
        "            row = cur[i,:,0,0].clone().zero_()\n",
        "            keep = (alive & (W2[i] >= thr))\n",
        "            row[keep] = 1.0\n",
        "            cur[i,:,0,0].copy_(row)\n",
        "        for i in freq:\n",
        "            apply_row(i, self.d_f)\n",
        "        for i in rare:\n",
        "            apply_row(i, self.d_r)\n",
        "        self.mask.data.copy_(cur)\n",
        "\n",
        "    def forward(self, x):\n",
        "        m = self.mask.to(dtype=self.conv.weight.dtype, device=self.conv.weight.device)\n",
        "        w = self.conv.weight * m.expand_as(self.conv.weight)\n",
        "        pre = F.conv2d(x, w, self.conv.bias, stride=self.conv.stride,\n",
        "                       padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups)\n",
        "        pre = self._rescale(pre)\n",
        "        h = F.relu(pre, inplace=False)\n",
        "        if self.bn is not None:\n",
        "            h = self.bn(h)\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                act = (h > 0).to(h.dtype).mean(dim=(0,2,3))\n",
        "                self.activation_counts.mul_(1 - self.alpha).add_(self.alpha * act)\n",
        "                self.step += 1\n",
        "                self._counter += 1\n",
        "                if int(self.step.item()) >= self.warmup_steps and self.update_interval > 0 and (self._counter % self.update_interval == 0):\n",
        "                    self._update_mask()\n",
        "        return h\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, use_sp=False, sp_kwargs=None):\n",
        "        super().__init__()\n",
        "        sp_kwargs = sp_kwargs or {}\n",
        "        if use_sp:\n",
        "            self.c1 = SPInConv2d(in_ch, out_ch, k=3, p=1, use_bn=True, **sp_kwargs)\n",
        "        else:\n",
        "            self.c1 = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        self.c2 = nn.Sequential(\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.c1(x)\n",
        "        x = self.c2(x)\n",
        "        return x\n",
        "\n",
        "class SiameseEncoder(nn.Module):\n",
        "    def __init__(self, in_ch=3, feats=(32,64,128,256), use_sp=False, sp_kwargs=None):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.pools = nn.ModuleList()\n",
        "        ch = in_ch\n",
        "        for f in feats:\n",
        "            self.blocks.append(ConvBlock(ch, f, use_sp=use_sp, sp_kwargs=sp_kwargs))\n",
        "            self.pools.append(nn.MaxPool2d(2))\n",
        "            ch = f\n",
        "    def forward_once(self, x):\n",
        "        feats = []\n",
        "        for block, pool in zip(self.blocks, self.pools):\n",
        "            x = block(x); feats.append(x); x = pool(x)\n",
        "        return feats, x\n",
        "    def forward(self, a, b):\n",
        "        fa, xa = self.forward_once(a)\n",
        "        fb, xb = self.forward_once(b)\n",
        "        return fa, xa, fb, xb\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, feats=(32,64,128,256)):\n",
        "        super().__init__()\n",
        "        self.upconvs = nn.ModuleList()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        ch = feats[-1]*2\n",
        "        for f in reversed(feats):\n",
        "            self.upconvs.append(nn.ConvTranspose2d(ch, f, 2, stride=2))\n",
        "            self.blocks.append(ConvBlock(f*3, f, use_sp=False))\n",
        "            ch = f\n",
        "        self.final = nn.Conv2d(feats[0], 1, 1)\n",
        "    def forward(self, fa, xa, fb, xb):\n",
        "        x = torch.cat([xa, xb], dim=1)\n",
        "        for i in range(len(self.upconvs)):\n",
        "            x = self.upconvs[i](x)\n",
        "            fa_i = fa[-(i+1)]\n",
        "            fb_i = fb[-(i+1)]\n",
        "            x = torch.cat([x, fa_i, fb_i], dim=1)\n",
        "            x = self.blocks[i](x)\n",
        "        return self.final(x)\n",
        "\n",
        "class FCSiamConc(nn.Module):\n",
        "    def __init__(self, in_ch=3, feats=(32,64,128,256), use_sp=False, sp_kwargs=None):\n",
        "        super().__init__()\n",
        "        self.encoder = SiameseEncoder(in_ch=in_ch, feats=feats, use_sp=use_sp, sp_kwargs=sp_kwargs)\n",
        "        self.decoder = Decoder(feats=feats)\n",
        "    def forward(self, a, b):\n",
        "        fa, xa, fb, xb = self.encoder(a, b)\n",
        "        logits = self.decoder(fa, xa, fb, xb)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eE2SbwQoVquW",
      "metadata": {
        "id": "eE2SbwQoVquW"
      },
      "source": [
        "## 4) Metrics & helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "166JMlf-VquX",
      "metadata": {
        "id": "166JMlf-VquX"
      },
      "outputs": [],
      "source": [
        "def binarize(logits: torch.Tensor, thr: float = 0.5) -> torch.Tensor:\n",
        "    return (torch.sigmoid(logits) >= thr).to(torch.uint8)\n",
        "\n",
        "def iou_f1(pred: np.ndarray, gt: np.ndarray):\n",
        "    inter = np.logical_and(pred==1, gt==1).sum()\n",
        "    union = np.logical_or(pred==1, gt==1).sum()\n",
        "    iou = inter / (union + 1e-6)\n",
        "    tp = inter\n",
        "    fp = np.logical_and(pred==1, gt==0).sum()\n",
        "    fn = np.logical_and(pred==0, gt==1).sum()\n",
        "    f1 = (2*tp) / (2*tp + fp + fn + 1e-6)\n",
        "    return float(iou), float(f1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, thr=0.5):\n",
        "    model.eval()\n",
        "    iou_all, f1_all = [], []\n",
        "    for a,b,y,_ in loader:\n",
        "        a,b,y = a.to(device), b.to(device), y.to(device)\n",
        "        logits = model(a,b)\n",
        "        pred = binarize(logits, thr=thr).cpu().numpy().astype(np.uint8)\n",
        "        gt = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        for i in range(pred.shape[0]):\n",
        "            iou, f1 = iou_f1(pred[i,0], gt[i,0])\n",
        "            iou_all.append(iou); f1_all.append(f1)\n",
        "    return {\"mean_IoU\": float(np.mean(iou_all)) if iou_all else 0.0,\n",
        "            \"mean_F1\": float(np.mean(f1_all)) if f1_all else 0.0}\n",
        "\n",
        "def overlay_example(img_rgb: np.ndarray, gt: np.ndarray, pred_dense: np.ndarray, pred_sp: np.ndarray):\n",
        "    # img_rgb: HxWx3 uint8, gt/preds: HxW {0,1}\n",
        "    base = img_rgb.copy()\n",
        "    h,w,_ = base.shape\n",
        "    canvas = base.astype(np.float32)\n",
        "    both = (pred_dense==1) & (pred_sp==1) & (gt==1)   # yellow\n",
        "    sp_only = (pred_sp==1) & (pred_dense==0) & (gt==1)  # green\n",
        "    dense_only = (pred_dense==1) & (pred_sp==0) & (gt==1) # red\n",
        "    # overlay with 50% alpha\n",
        "    def paint(mask, color):\n",
        "        c = np.array(color, dtype=np.float32)[None,None,:]\n",
        "        m = mask.astype(np.float32)[:,:,None]\n",
        "        canvas = base.astype(np.float32)\n",
        "        return np.where(m>0, 0.5*canvas + 0.5*c, canvas)\n",
        "    tmp = paint(both, (255,255,0))\n",
        "    tmp = np.where(sp_only[:,:,None]>0, 0.5*tmp + 0.5*np.array([0,255,0],dtype=np.float32), tmp)\n",
        "    tmp = np.where(dense_only[:,:,None]>0, 0.5*tmp + 0.5*np.array([255,0,0],dtype=np.float32), tmp)\n",
        "    return tmp.clip(0,255).astype(np.uint8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vve81kvtVquX",
      "metadata": {
        "id": "vve81kvtVquX"
      },
      "source": [
        "## 5) Train loops (Dense / SP-in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oGa_VSmtVquX",
      "metadata": {
        "id": "oGa_VSmtVquX"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device, epoch=None, total_epochs=None, method=None):\n",
        "    model.train()\n",
        "    loss_meter, n = 0.0, 0\n",
        "    desc = f\"Train[{method}] {epoch}/{total_epochs}\" if method else \"Train\"\n",
        "    pbar = tqdm(loader, desc=desc, dynamic_ncols=True, leave=True)\n",
        "    for a,b,y,_ in pbar:\n",
        "        a,b,y = a.to(device), b.to(device), y.to(device)\n",
        "        logits = model(a,b)\n",
        "        loss = F.binary_cross_entropy_with_logits(logits, y)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "\n",
        "        bs = a.size(0)\n",
        "        loss_meter += float(loss.item()) * bs\n",
        "        n += bs\n",
        "\n",
        "        # show rolling average & current lr on progress bar\n",
        "        avg = loss_meter / max(1,n)\n",
        "        lr  = optimizer.param_groups[0]['lr']\n",
        "        pbar.set_postfix(loss=f\"{avg:.4f}\", lr=f\"{lr:.4g}\")\n",
        "    pbar.close()\n",
        "    return loss_meter / max(1,n)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, thr=0.5, desc=None):\n",
        "    \"\"\"Evaluate without bucketing (LEVIR-CD style metrics).\"\"\"\n",
        "    model.eval()\n",
        "    iou_all, f1_all = [], []\n",
        "    all_preds, all_gts, all_names = [], [], []\n",
        "    it = tqdm(loader, desc=desc, dynamic_ncols=True, leave=True) if desc else loader\n",
        "    for a,b,y,names in it:\n",
        "        a,b,y = a.to(device), b.to(device), y.to(device)\n",
        "        logits = model(a,b)\n",
        "        pred = (torch.sigmoid(logits) >= thr).to(torch.uint8).cpu().numpy()\n",
        "        gt   = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        for i in range(pred.shape[0]):\n",
        "            inter = np.logical_and(pred[i,0]==1, gt[i,0]==1).sum()\n",
        "            union = np.logical_or (pred[i,0]==1, gt[i,0]==1).sum()\n",
        "            iou = inter / (union + 1e-6)\n",
        "            tp  = inter\n",
        "            fp  = np.logical_and(pred[i,0]==1, gt[i,0]==0).sum()\n",
        "            fn  = np.logical_and(pred[i,0]==0, gt[i,0]==1).sum()\n",
        "            f1  = (2*tp) / (2*tp + fp + fn + 1e-6)\n",
        "            iou_all.append(float(iou)); f1_all.append(float(f1))\n",
        "        all_preds.append(pred[:,0])\n",
        "        all_gts.append(gt[:,0])\n",
        "        all_names.extend(list(names))\n",
        "    return {\n",
        "        \"mean_IoU\": float(np.mean(iou_all)) if iou_all else 0.0,\n",
        "        \"mean_F1\":  float(np.mean(f1_all))  if f1_all  else 0.0,\n",
        "        \"preds\": np.concatenate(all_preds, axis=0) if all_preds else None,\n",
        "        \"gts\":   np.concatenate(all_gts,   axis=0) if all_gts   else None,\n",
        "        \"names\": all_names,\n",
        "    }\n",
        "def train_and_eval(method: str, use_sp: bool, epochs: int, run_subdir: str, sp_kwargs=None):\n",
        "    sp_kwargs = sp_kwargs or {}\n",
        "    save_dir = os.path.join(RUN_DIR, run_subdir)\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    model = FCSiamConc(in_ch=3, feats=(32,64,128,256), use_sp=use_sp, sp_kwargs=sp_kwargs).to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "    best_f1, best_path = -1.0, os.path.join(save_dir, \"best.pth\")\n",
        "    last_path = os.path.join(save_dir, \"last.pth\")\n",
        "    log = []\n",
        "    for ep in range(1, epochs+1):\n",
        "        tr_loss = train_one_epoch(model, train_loader, optim, device,\n",
        "                                  epoch=ep, total_epochs=epochs, method=method)\n",
        "        val_res = evaluate(model, val_loader, device, desc=f\"Val[{method}] {ep}/{epochs}\")\n",
        "        log.append({\"epoch\": ep, \"train_loss\": tr_loss, **{k:v for k,v in val_res.items() if isinstance(v, float)}})\n",
        "        from tqdm import tqdm as _tqdm\n",
        "        _tqdm.write(f\"[{method}] Epoch {ep:03d} | Train {tr_loss:.4f} | Val F1 {val_res['mean_F1']:.4f} | IoU {val_res['mean_IoU']:.4f}\")\n",
        "\n",
        "        torch.save(model.state_dict(), last_path)\n",
        "        if val_res[\"mean_F1\"] > best_f1:\n",
        "            best_f1 = val_res[\"mean_F1\"]\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            with open(os.path.join(save_dir, \"val_best.json\"), \"w\") as f:\n",
        "                json.dump({\"epoch\": ep, **val_res}, f, indent=2)\n",
        "        with open(os.path.join(save_dir, \"epoch_log.jsonl\"), \"a\") as f:\n",
        "            f.write(json.dumps(log[-1]) + \"\\n\")\n",
        "\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    test_res = evaluate(model, test_loader, device, desc=f\"Test[{method}]\")\n",
        "    with open(os.path.join(save_dir, \"test_metrics.json\"), \"w\") as f:\n",
        "        json.dump(test_res, f, indent=2)\n",
        "    print(f\"[{method}] Test F1 {test_res['mean_F1']:.4f} | IoU {test_res['mean_IoU']:.4f}\")\n",
        "    return save_dir, model, test_res\n",
        "\n",
        "\n",
        "# SP-in default hyperparams (safe starting point)\n",
        "SP_KW = dict(\n",
        "    alpha=0.05, update_interval=25, warmup_steps=40,\n",
        "    tau_high=0.7, tau_low=0.1,\n",
        "    dens_freq=0.4, dens_rare=0.9,\n",
        "    min_fan=4, rescale_mode='sqrt'\n",
        ")\n",
        "\n",
        "dense_dir, dense_model, dense_test = train_and_eval(\"dense\", use_sp=False, epochs=EPOCHS_DENSE, run_subdir=\"dense\")\n",
        "sp_dir, sp_model, sp_test = train_and_eval(\"sp_in\", use_sp=True, epochs=EPOCHS_SP, run_subdir=\"sp_in\", sp_kwargs=SP_KW)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "odcGGWs_VquX",
      "metadata": {
        "id": "odcGGWs_VquX"
      },
      "source": [
        "## 6) Compare on test & produce a “best example” overlay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_qTNiMoHVquX",
      "metadata": {
        "id": "_qTNiMoHVquX"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def pick_best_example_and_save(dense_model, sp_model, loader, save_dir):\n",
        "    dense_model.eval(); sp_model.eval()\n",
        "    best_score = -1e9\n",
        "    best_path = None\n",
        "    best_img = None\n",
        "    for a,b,y,names in loader:\n",
        "        a,b,y = a.to(device), b.to(device), y.to(device)\n",
        "        logits_d = dense_model(a,b)\n",
        "        logits_s = sp_model(a,b)\n",
        "        pred_d = binarize(logits_d, 0.5).cpu().numpy().astype(np.uint8)\n",
        "        pred_s = binarize(logits_s, 0.5).cpu().numpy().astype(np.uint8)\n",
        "        gt = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        a_np = (a.cpu().numpy()*255).astype(np.uint8).transpose(0,2,3,1)\n",
        "        b_np = (b.cpu().numpy()*255).astype(np.uint8).transpose(0,2,3,1)\n",
        "        for i in range(pred_d.shape[0]):\n",
        "            # score: sp-only hits minus dense-only hits\n",
        "            sp_only = np.logical_and(pred_s[i,0]==1, np.logical_and(pred_d[i,0]==0, gt[i,0]==1)).sum()\n",
        "            dense_only = np.logical_and(pred_d[i,0]==1, np.logical_and(pred_s[i,0]==0, gt[i,0]==1)).sum()\n",
        "            score = sp_only - dense_only\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_path = names[i]\n",
        "                overlay = overlay_example(b_np[i], gt[i,0], pred_d[i,0], pred_s[i,0])\n",
        "                best_img = overlay\n",
        "    if best_img is not None:\n",
        "        out_png = os.path.join(RUN_DIR, \"best_example_overlay.png\")\n",
        "        Image.fromarray(best_img).save(out_png)\n",
        "        return out_png, best_path, int(best_score)\n",
        "    return None, None, 0\n",
        "\n",
        "overlay_png, sample_name, score = pick_best_example_and_save(dense_model, sp_model, test_loader, RUN_DIR)\n",
        "summary = {\n",
        "    \"dense_test\": dense_test,\n",
        "    \"sp_in_test\": sp_test,\n",
        "    \"best_example\": {\n",
        "        \"file\": overlay_png,\n",
        "        \"sample\": sample_name,\n",
        "        \"score_sp_only_minus_dense_only\": score\n",
        "    }\n",
        "}\n",
        "with open(os.path.join(RUN_DIR, \"comparison_summary.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(\"Saved comparison summary to:\", os.path.join(RUN_DIR, \"comparison_summary.json\"))\n",
        "print(\"Best example overlay:\", overlay_png)\n",
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebIMt9nVZQ1V",
      "metadata": {
        "id": "ebIMt9nVZQ1V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wpiNMMnJZQy9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpiNMMnJZQy9",
        "outputId": "66478068-d42a-49b7-9404-dfabbd478178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[OK] Found LEVIR-CD at: /content/drive/MyDrive/LEVIR-CD\n",
            "\n",
            "==== Train: DENSE ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 01/30 | TrainLoss 0.5161 | Val F1 0.0710 | Val IoU 0.0530\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 02/30 | TrainLoss 0.3102 | Val F1 0.0391 | Val IoU 0.0247\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 03/30 | TrainLoss 0.2180 | Val F1 0.1702 | Val IoU 0.1009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 04/30 | TrainLoss 0.1644 | Val F1 0.3599 | Val IoU 0.2609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 05/30 | TrainLoss 0.1324 | Val F1 0.2828 | Val IoU 0.1929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 06/30 | TrainLoss 0.1091 | Val F1 0.3303 | Val IoU 0.2262\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 07/30 | TrainLoss 0.0972 | Val F1 0.3742 | Val IoU 0.2666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 08/30 | TrainLoss 0.0872 | Val F1 0.4008 | Val IoU 0.2913\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 09/30 | TrainLoss 0.0803 | Val F1 0.4443 | Val IoU 0.3300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 10/30 | TrainLoss 0.0759 | Val F1 0.4863 | Val IoU 0.3774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 11/30 | TrainLoss 0.0678 | Val F1 0.5090 | Val IoU 0.3953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 12/30 | TrainLoss 0.0662 | Val F1 0.3039 | Val IoU 0.2141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 13/30 | TrainLoss 0.0681 | Val F1 0.4398 | Val IoU 0.3249\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 14/30 | TrainLoss 0.0614 | Val F1 0.5518 | Val IoU 0.4409\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 15/30 | TrainLoss 0.0567 | Val F1 0.4938 | Val IoU 0.3836\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 16/30 | TrainLoss 0.0536 | Val F1 0.4904 | Val IoU 0.3831\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 17/30 | TrainLoss 0.0521 | Val F1 0.5310 | Val IoU 0.4245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 18/30 | TrainLoss 0.0513 | Val F1 0.5716 | Val IoU 0.4682\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 19/30 | TrainLoss 0.0492 | Val F1 0.5723 | Val IoU 0.4703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 20/30 | TrainLoss 0.0479 | Val F1 0.5239 | Val IoU 0.4302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 21/30 | TrainLoss 0.0485 | Val F1 0.5849 | Val IoU 0.4820\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 22/30 | TrainLoss 0.0463 | Val F1 0.5734 | Val IoU 0.4720\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 23/30 | TrainLoss 0.0455 | Val F1 0.5711 | Val IoU 0.4674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 24/30 | TrainLoss 0.0424 | Val F1 0.5574 | Val IoU 0.4515\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 25/30 | TrainLoss 0.0438 | Val F1 0.5777 | Val IoU 0.4735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 26/30 | TrainLoss 0.0432 | Val F1 0.5681 | Val IoU 0.4643\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 27/30 | TrainLoss 0.0408 | Val F1 0.5952 | Val IoU 0.4977\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 28/30 | TrainLoss 0.0411 | Val F1 0.5777 | Val IoU 0.4758\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 29/30 | TrainLoss 0.0399 | Val F1 0.5969 | Val IoU 0.5001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Dense] Epoch 30/30 | TrainLoss 0.0387 | Val F1 0.5617 | Val IoU 0.4559\n",
            "==> Dense Test: {'mean_IoU': 0.5344, 'mean_F1': 0.6418}\n",
            "\n",
            "==== Train: SP-in ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 01/30 | TrainLoss 0.4848 | Val F1 0.0056 | Val IoU 0.0030\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 02/30 | TrainLoss 0.3006 | Val F1 0.0070 | Val IoU 0.0038\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 03/30 | TrainLoss 0.2097 | Val F1 0.0001 | Val IoU 0.0001\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 04/30 | TrainLoss 0.1581 | Val F1 0.2738 | Val IoU 0.1826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 05/30 | TrainLoss 0.1277 | Val F1 0.3569 | Val IoU 0.2508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 06/30 | TrainLoss 0.1059 | Val F1 0.3568 | Val IoU 0.2525\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 07/30 | TrainLoss 0.0948 | Val F1 0.4269 | Val IoU 0.3209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 08/30 | TrainLoss 0.0865 | Val F1 0.3628 | Val IoU 0.2612\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 09/30 | TrainLoss 0.0794 | Val F1 0.4989 | Val IoU 0.3891\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 10/30 | TrainLoss 0.0717 | Val F1 0.2863 | Val IoU 0.2033\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 11/30 | TrainLoss 0.0683 | Val F1 0.4682 | Val IoU 0.3614\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 12/30 | TrainLoss 0.0647 | Val F1 0.4760 | Val IoU 0.3666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 13/30 | TrainLoss 0.0623 | Val F1 0.5048 | Val IoU 0.3971\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 14/30 | TrainLoss 0.0563 | Val F1 0.5351 | Val IoU 0.4287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 15/30 | TrainLoss 0.0555 | Val F1 0.5355 | Val IoU 0.4260\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 16/30 | TrainLoss 0.0527 | Val F1 0.5709 | Val IoU 0.4676\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 17/30 | TrainLoss 0.0500 | Val F1 0.5534 | Val IoU 0.4483\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 18/30 | TrainLoss 0.0493 | Val F1 0.5648 | Val IoU 0.4594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 19/30 | TrainLoss 0.0485 | Val F1 0.5551 | Val IoU 0.4546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 20/30 | TrainLoss 0.0434 | Val F1 0.5779 | Val IoU 0.4817\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 21/30 | TrainLoss 0.0444 | Val F1 0.5385 | Val IoU 0.4367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 22/30 | TrainLoss 0.0427 | Val F1 0.5904 | Val IoU 0.4909\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 23/30 | TrainLoss 0.0452 | Val F1 0.5121 | Val IoU 0.4127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 24/30 | TrainLoss 0.0429 | Val F1 0.5959 | Val IoU 0.5004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 25/30 | TrainLoss 0.0404 | Val F1 0.6024 | Val IoU 0.5060\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 26/30 | TrainLoss 0.0398 | Val F1 0.6048 | Val IoU 0.5079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 27/30 | TrainLoss 0.0385 | Val F1 0.6099 | Val IoU 0.5111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 28/30 | TrainLoss 0.0368 | Val F1 0.6029 | Val IoU 0.5031\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 29/30 | TrainLoss 0.0370 | Val F1 0.6081 | Val IoU 0.5105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SP-in] Epoch 30/30 | TrainLoss 0.0359 | Val F1 0.5753 | Val IoU 0.4790\n",
            "==> SP-in Test: {'mean_IoU': 0.5373, 'mean_F1': 0.6456}\n",
            "[Viz] Selected top-10 indices by ΔTP: [68, 69, 107, 106, 55, 33, 61, 66, 103, 18]\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_top10/panel_rank01_dTP2609_test_45.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_top10/panel_rank02_dTP1624_test_46.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_top10/panel_rank03_dTP1203_test_80.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_top10/panel_rank04_dTP1132_test_8.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_top10/panel_rank05_dTP790_test_33.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_top10/panel_rank06_dTP758_test_13.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_top10/panel_rank07_dTP693_test_39.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_top10/panel_rank08_dTP624_test_43.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_top10/panel_rank09_dTP613_test_77.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_top10/panel_rank10_dTP541_test_115.png\n",
            "[Saved] summary -> /content/drive/MyDrive/LEVIR_CD_runs/summary.json\n"
          ]
        }
      ],
      "source": [
        "# @title LEVIR-CD | Dense vs SP-in (30 epochs each) – Colab-ready\n",
        "# -*- coding: utf-8 -*-\n",
        "import os, sys, math, time, random, json\n",
        "from typing import Optional, Tuple, List, Dict\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Configs (edit here if needed)\n",
        "# =========================\n",
        "CFG = {\n",
        "    \"root\": \"/content/drive/MyDrive/LEVIR-CD\",  # <- change to your LEVIR-CD root directory (containing train/val/test three subdirectories)\n",
        "    \"resize\": 256,\n",
        "    \"batch_size\": 8,\n",
        "    \"epochs_per_method\": 30,\n",
        "    \"lr\": 1e-3,\n",
        "    \"seed\": 42,\n",
        "    \"out_dir\": \"/content/drive/MyDrive/LEVIR_CD_runs\",  # save results\n",
        "    # SP-in hyperparameters (conservative default)\n",
        "    \"sp\": {\n",
        "        \"alpha\": 0.05,\n",
        "        \"update_interval\": 10,   # update mask every 50 batches\n",
        "        \"warmup_steps\": 25,    # before 1000 steps, mask is not updated\n",
        "        \"tau_high\": 0.7,         # frequent activation threshold (static threshold when percentile is difficult)\n",
        "        \"tau_low\": 0.3,          # rare activation threshold\n",
        "        \"dens_freq\": 0.5,        # frequent channels: more sparse\n",
        "        \"dens_rare\": 0.9,        # rare channels: more dense\n",
        "        \"min_fan\": 4,\n",
        "        \"rescale_mode\": \"sqrt\",\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Colab helpers\n",
        "# =========================\n",
        "def in_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab  # type: ignore\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def mount_drive_if_colab():\n",
        "    if in_colab():\n",
        "        from google.colab import drive  # type: ignore\n",
        "        try:\n",
        "            drive.mount(\"/content/drive\", force_remount=False)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Repro\n",
        "# =========================\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Dataset\n",
        "# =========================\n",
        "class LevirCD(Dataset):\n",
        "    \"\"\"\n",
        "    directory structure requirements:\n",
        "      root/\n",
        "        train/ A|B|label\n",
        "        val/   A|B|label\n",
        "        test/  A|B|label\n",
        "    each subfolder has corresponding file names.\n",
        "    \"\"\"\n",
        "    def __init__(self, root: str, split: str = \"train\", resize: int = 256, aug: bool = True):\n",
        "        super().__init__()\n",
        "        base = os.path.join(root, split)\n",
        "        a_dir = os.path.join(base, \"A\")\n",
        "        b_dir = os.path.join(base, \"B\")\n",
        "        l_dir = os.path.join(base, \"label\")\n",
        "        if not (os.path.isdir(a_dir) and os.path.isdir(b_dir) and os.path.isdir(l_dir)):\n",
        "            raise FileNotFoundError(f\"[{split}] expected folders A/B/label under: {base}\")\n",
        "\n",
        "        names = set(os.listdir(a_dir)) & set(os.listdir(b_dir)) & set(os.listdir(l_dir))\n",
        "        self.names = sorted([n for n in names if n.lower().endswith((\".png\",\".jpg\",\".jpeg\",\".tif\",\".bmp\"))])\n",
        "        if len(self.names) == 0:\n",
        "            raise FileNotFoundError(f\"No images found under {base}/A|B|label\")\n",
        "        self.a_dir, self.b_dir, self.l_dir = a_dir, b_dir, l_dir\n",
        "        self.resize = resize\n",
        "        self.aug = aug and (split == \"train\")\n",
        "\n",
        "    def __len__(self): return len(self.names)\n",
        "\n",
        "    def _load_img(self, path: str) -> np.ndarray:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.resize is not None:\n",
        "            img = img.resize((self.resize, self.resize), Image.BILINEAR)\n",
        "        return np.array(img, dtype=np.uint8)\n",
        "\n",
        "    def _load_mask(self, path: str) -> np.ndarray:\n",
        "        m = Image.open(path).convert(\"L\")\n",
        "        if self.resize is not None:\n",
        "            m = m.resize((self.resize, self.resize), Image.NEAREST)\n",
        "        arr = np.array(m, dtype=np.uint8)\n",
        "        if arr.max() > 1:\n",
        "            arr = (arr > 127).astype(np.uint8)\n",
        "        return arr\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        name = self.names[idx]\n",
        "        a = self._load_img(os.path.join(self.a_dir, name))\n",
        "        b = self._load_img(os.path.join(self.b_dir, name))\n",
        "        y = self._load_mask(os.path.join(self.l_dir, name))\n",
        "\n",
        "        if self.aug:\n",
        "            if np.random.rand() < 0.5:\n",
        "                a = np.flip(a, axis=1).copy(); b = np.flip(b, axis=1).copy(); y = np.flip(y, axis=1).copy()\n",
        "            if np.random.rand() < 0.5:\n",
        "                a = np.flip(a, axis=0).copy(); b = np.flip(b, axis=0).copy(); y = np.flip(y, axis=0).copy()\n",
        "\n",
        "        a = torch.from_numpy(a).permute(2,0,1).float() / 255.0\n",
        "        b = torch.from_numpy(b).permute(2,0,1).float() / 255.0\n",
        "        y = torch.from_numpy(y).float().unsqueeze(0)\n",
        "        return a, b, y, name\n",
        "\n",
        "\n",
        "# =========================\n",
        "# SP-in for Conv2d (fan-in mask)\n",
        "# =========================\n",
        "class SPInConv2d(nn.Module):\n",
        "    \"\"\"\n",
        "    dynamic sparse for fan-in (input channel dimension) of Conv2d:\n",
        "      - count the ratio of activated channels after ReLU (EMA)\n",
        "      - frequent channels use smaller density dens_freq, rare channels use larger density dens_rare\n",
        "      - only do top-k on \"current alive\" fan-in (prune-only), not grow; more stable\n",
        "      - mask shape: [out, in, 1, 1], broadcast to kernel\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch, out_ch, k=3, s=1, p=1, bias=True,\n",
        "                 alpha=0.05, update_interval=50, warmup_steps=1000,\n",
        "                 tau_high=0.7, tau_low=0.1, dens_freq=0.4, dens_rare=0.9,\n",
        "                 min_fan=4, rescale_mode='sqrt', use_bn=True):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, k, s, p, bias=bias)\n",
        "        self.use_bn = use_bn\n",
        "        self.bn = nn.BatchNorm2d(out_ch) if use_bn else None\n",
        "\n",
        "        self.alpha = float(alpha)\n",
        "        self.update_interval = int(update_interval)\n",
        "        self.warmup_steps = int(warmup_steps)\n",
        "        self.tau_h = float(tau_high)\n",
        "        self.tau_l = float(tau_low)\n",
        "        self.d_f = float(dens_freq)\n",
        "        self.d_r = float(dens_rare)\n",
        "        self.min_fan = int(min_fan)\n",
        "        self.rescale_mode = rescale_mode\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.ones(out_ch, in_ch, 1, 1))\n",
        "        self.register_buffer(\"act_ema\", torch.zeros(out_ch))\n",
        "        self.register_buffer(\"step\", torch.zeros((), dtype=torch.long), persistent=False)\n",
        "        self._tick = 0\n",
        "\n",
        "    def _rescale(self, pre):\n",
        "        if self.rescale_mode == \"none\":\n",
        "            return pre\n",
        "        # number of fan-in retained for each output channel\n",
        "        fan = self.mask.view(self.mask.size(0), -1).sum(dim=1).clamp_min(1.0)\n",
        "        base = float(self.mask.size(1))\n",
        "        if self.rescale_mode == \"linear\":\n",
        "            s = base / fan\n",
        "        else:\n",
        "            s = (base / fan).sqrt()\n",
        "        return pre * s.view(1, -1, 1, 1).to(pre.device, pre.dtype)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _update_mask(self):\n",
        "        # only update after warmup and every update_interval steps\n",
        "        if int(self.step.item()) < self.warmup_steps: return\n",
        "        if self.update_interval <= 0: return\n",
        "        if (self._tick % self.update_interval) != 0: return\n",
        "\n",
        "        cur = self.mask.detach().clone()                     # [out,in,1,1]\n",
        "        W = self.conv.weight.detach().abs()                  # [out,in,kh,kw]\n",
        "        W2 = W.view(W.size(0), W.size(1), -1).mean(dim=2)    # [out,in]\n",
        "        out_ch, in_ch = cur.size(0), cur.size(1)\n",
        "\n",
        "        freq = (self.act_ema > self.tau_h).nonzero(as_tuple=True)[0].tolist()\n",
        "        rare = (self.act_ema < self.tau_l).nonzero(as_tuple=True)[0].tolist()\n",
        "\n",
        "        def apply_row(i, density):\n",
        "            alive = cur[i,:,0,0].bool()\n",
        "            k = max(self.min_fan, int(round(in_ch * float(density))))\n",
        "            k = min(k, int(alive.sum().item()))\n",
        "            if k <= 0: return\n",
        "            cand = W2[i][alive]\n",
        "            if cand.numel() <= k:\n",
        "                return\n",
        "            thr = cand.topk(k, largest=True).values[-1]\n",
        "            row = cur[i,:,0,0].clone().zero_()\n",
        "            keep = (alive & (W2[i] >= thr))\n",
        "            row[keep] = 1.0\n",
        "            cur[i,:,0,0].copy_(row)\n",
        "\n",
        "        for i in freq: apply_row(i, self.d_f)\n",
        "        for i in rare: apply_row(i, self.d_r)\n",
        "\n",
        "        self.mask.data.copy_(cur)\n",
        "\n",
        "    def forward(self, x):\n",
        "        m = self.mask.to(dtype=self.conv.weight.dtype, device=self.conv.weight.device)\n",
        "        w = self.conv.weight * m.expand_as(self.conv.weight)\n",
        "        pre = F.conv2d(x, w, self.conv.bias, stride=self.conv.stride,\n",
        "                       padding=self.conv.padding, dilation=self.conv.dilation, groups=self.conv.groups)\n",
        "        pre = self._rescale(pre)\n",
        "        h = F.relu(pre, inplace=False)\n",
        "        if self.use_bn: h = self.bn(h)\n",
        "\n",
        "        if self.training:\n",
        "            with torch.no_grad():\n",
        "                # count the ratio of activated channels after ReLU (EMA)\n",
        "                act = (h > 0).to(h.dtype).mean(dim=(0,2,3))\n",
        "                self.act_ema.mul_(1 - self.alpha).add_(self.alpha * act)\n",
        "                self.step += 1\n",
        "                self._tick += 1\n",
        "                self._update_mask()\n",
        "        return h\n",
        "\n",
        "\n",
        "# =========================\n",
        "# FC-Siam-conc\n",
        "# =========================\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, use_sp=False, sp_kwargs=None):\n",
        "        super().__init__()\n",
        "        sp_kwargs = sp_kwargs or {}\n",
        "        if use_sp:\n",
        "            self.c1 = SPInConv2d(in_ch, out_ch, k=3, p=1, use_bn=True, **sp_kwargs)\n",
        "        else:\n",
        "            self.c1 = nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "            )\n",
        "        self.c2 = nn.Sequential(\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.c2(self.c1(x))\n",
        "\n",
        "\n",
        "class SiameseEncoder(nn.Module):\n",
        "    def __init__(self, in_ch=3, feats=(32,64,128,256), use_sp=False, sp_kwargs=None):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList()\n",
        "        self.pools  = nn.ModuleList()\n",
        "        ch = in_ch\n",
        "        for f in feats:\n",
        "            self.blocks.append(ConvBlock(ch, f, use_sp=use_sp, sp_kwargs=sp_kwargs))\n",
        "            self.pools.append(nn.MaxPool2d(2))\n",
        "            ch = f\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        feats = []\n",
        "        for blk, pool in zip(self.blocks, self.pools):\n",
        "            x = blk(x); feats.append(x); x = pool(x)\n",
        "        return feats, x\n",
        "\n",
        "    def forward(self, a, b):\n",
        "        fa, xa = self.forward_once(a)\n",
        "        fb, xb = self.forward_once(b)\n",
        "        return fa, xa, fb, xb\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, feats=(32,64,128,256)):\n",
        "        super().__init__()\n",
        "        self.up  = nn.ModuleList()\n",
        "        self.blk = nn.ModuleList()\n",
        "        ch = feats[-1]*2\n",
        "        for f in reversed(feats):\n",
        "            self.up.append(nn.ConvTranspose2d(ch, f, 2, stride=2))\n",
        "            self.blk.append(ConvBlock(f*3, f, use_sp=False))\n",
        "            ch = f\n",
        "        self.final = nn.Conv2d(feats[0], 1, 1)\n",
        "\n",
        "    def forward(self, fa, xa, fb, xb):\n",
        "        x = torch.cat([xa, xb], dim=1)\n",
        "        for i in range(len(self.up)):\n",
        "            x = self.up[i](x)\n",
        "            fa_i = fa[-(i+1)]; fb_i = fb[-(i+1)]\n",
        "            x = torch.cat([x, fa_i, fb_i], dim=1)\n",
        "            x = self.blk[i](x)\n",
        "        return self.final(x)\n",
        "\n",
        "\n",
        "class FCSiamConc(nn.Module):\n",
        "    def __init__(self, in_ch=3, feats=(32,64,128,256), use_sp=False, sp_kwargs=None):\n",
        "        super().__init__()\n",
        "        self.encoder = SiameseEncoder(in_ch=in_ch, feats=feats, use_sp=use_sp, sp_kwargs=sp_kwargs)\n",
        "        self.decoder = Decoder(feats=feats)\n",
        "\n",
        "    def forward(self, a, b):\n",
        "        fa, xa, fb, xb = self.encoder(a, b)\n",
        "        return self.decoder(fa, xa, fb, xb)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Metrics & helpers\n",
        "# =========================\n",
        "def binarize(logits: torch.Tensor, thr: float = 0.5) -> torch.Tensor:\n",
        "    return (torch.sigmoid(logits) >= thr).to(torch.uint8)\n",
        "\n",
        "def iou_f1(pred: np.ndarray, gt: np.ndarray) -> Tuple[float, float]:\n",
        "    inter = np.logical_and(pred==1, gt==1).sum()\n",
        "    union = np.logical_or(pred==1, gt==1).sum()\n",
        "    iou = inter / (union + 1e-6)\n",
        "    tp = inter\n",
        "    fp = np.logical_and(pred==1, gt==0).sum()\n",
        "    fn = np.logical_and(pred==0, gt==1).sum()\n",
        "    f1 = (2*tp) / (2*tp + fp + fn + 1e-6)\n",
        "    return float(iou), float(f1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, thr=0.5):\n",
        "    model.eval()\n",
        "    ious, f1s = [], []\n",
        "    all_preds, all_gts, all_names = [], [], []\n",
        "    for a,b,y,names in loader:\n",
        "        a,b,y = a.to(device), b.to(device), y.to(device)\n",
        "        logits = model(a,b)\n",
        "        pred = binarize(logits, thr=thr).cpu().numpy().astype(np.uint8)\n",
        "        gt   = (y.cpu().numpy() > 0.5).astype(np.uint8)\n",
        "        for i in range(pred.shape[0]):\n",
        "            iou, f1 = iou_f1(pred[i,0], gt[i,0])\n",
        "            ious.append(iou); f1s.append(f1)\n",
        "        all_preds.append(pred[:,0])\n",
        "        all_gts.append(gt[:,0])\n",
        "        all_names.extend(list(names))\n",
        "    return {\n",
        "        \"mean_IoU\": float(np.mean(ious)) if ious else 0.0,\n",
        "        \"mean_F1\": float(np.mean(f1s)) if f1s else 0.0,\n",
        "        \"preds\": np.concatenate(all_preds, axis=0) if all_preds else None,\n",
        "        \"gts\":   np.concatenate(all_gts, axis=0) if all_gts else None,\n",
        "        \"names\": all_names,\n",
        "    }\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    loss_meter, n = 0.0, 0\n",
        "    for a,b,y,_ in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        a,b,y = a.to(device), b.to(device), y.to(device)\n",
        "        logits = model(a,b)\n",
        "        loss = F.binary_cross_entropy_with_logits(logits, y)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        loss_meter += float(loss.item()) * a.size(0)\n",
        "        n += a.size(0)\n",
        "    return loss_meter / max(1,n)\n",
        "\n",
        "def choose_perfect_example(a_dir, b_dir, name,\n",
        "                           gt: np.ndarray, pd_dense: np.ndarray, pd_spin: np.ndarray) -> Image.Image:\n",
        "    \"\"\"\n",
        "    color only in the region where GT==1:\n",
        "      green: SP-in hit and dense miss\n",
        "      red: dense hit and SP-in miss\n",
        "      blue: both hit\n",
        "      black: both miss (or GT==0)\n",
        "    background use B image (second phase).\n",
        "    \"\"\"\n",
        "    img = Image.open(os.path.join(b_dir, name)).convert(\"RGB\").resize(gt.shape[::-1], Image.BILINEAR)\n",
        "    base = np.array(img, dtype=np.uint8)\n",
        "\n",
        "    H,W = gt.shape\n",
        "    overlay = np.zeros((H,W,3), dtype=np.uint8)\n",
        "\n",
        "    tp_dense = (pd_dense==1) & (gt==1)\n",
        "    tp_spin  = (pd_spin==1)  & (gt==1)\n",
        "    both = tp_dense & tp_spin\n",
        "    only_dense = tp_dense & (~tp_spin)\n",
        "    only_spin  = tp_spin  & (~tp_dense)\n",
        "\n",
        "    # color\n",
        "    overlay[both]       = (0, 255, 255)   # blue\n",
        "    overlay[only_spin]  = (0, 255,   0)   # green\n",
        "    overlay[only_dense] = (255, 0,   0)   # red\n",
        "\n",
        "    # overlay\n",
        "    alpha = 0.5\n",
        "    vis = (base*(1-alpha) + overlay*alpha).astype(np.uint8)\n",
        "    # draw a legend\n",
        "    legend = np.ones((70, W, 3), dtype=np.uint8)*255\n",
        "    def put_patch(x0, color, text):\n",
        "        x1 = x0+160\n",
        "        c = np.array(color, dtype=np.uint8)\n",
        "        legend[10:40, x0:x0+40] = c\n",
        "        # simple text (use color block instead)\n",
        "        # to avoid dependency on PIL font, here use color block: c-legend\n",
        "        # you can use cv2.putText (Colab can install opencv)\n",
        "        return x1\n",
        "    x = 10\n",
        "    x = put_patch(x, (0,255,255), \"Both\")\n",
        "    x = put_patch(x, (0,255,0),   \"SP-in only\")\n",
        "    x = put_patch(x, (255,0,0),   \"Dense only\")\n",
        "    vis_full = np.vstack([legend, vis])\n",
        "    return Image.fromarray(vis_full)\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# load raw triplet (original resolution)\n",
        "def load_raw_triplet(root: str, split: str, name: str):\n",
        "    pa = os.path.join(root, split, \"A\", name)\n",
        "    pb = os.path.join(root, split, \"B\", name)\n",
        "    pg = os.path.join(root, split, \"label\", name)\n",
        "    A = Image.open(pa).convert(\"RGB\")\n",
        "    B = Image.open(pb).convert(\"RGB\")\n",
        "    G = Image.open(pg).convert(\"L\")\n",
        "    A = np.array(A, dtype=np.uint8)\n",
        "    B = np.array(B, dtype=np.uint8)\n",
        "    GT = (np.array(G, dtype=np.uint8) > 127).astype(np.uint8)\n",
        "    return A, B, GT  # A,B:[H,W,3]; GT:[H,W] in {0,1}\n",
        "\n",
        "# upsample 0/1 mask to target size\n",
        "def upsample_mask_to(mask_small: np.ndarray, target_hw: tuple[int,int]) -> np.ndarray:\n",
        "    H, W = target_hw\n",
        "    pil = Image.fromarray((mask_small.astype(np.uint8) * 255))\n",
        "    pil = pil.resize((W, H), Image.NEAREST)\n",
        "    return (np.array(pil, dtype=np.uint8) > 127).astype(np.uint8)\n",
        "\n",
        "# use small prediction to build \"original overlay\", and get \"colored GT\" by unmixing, then use colored GT to overlay for consistency check\n",
        "def make_overlay_and_colorized(B: np.ndarray, GT: np.ndarray,\n",
        "                               pred_dense_small: np.ndarray,\n",
        "                               pred_spin_small:  np.ndarray,\n",
        "                               alpha: float = 0.5):\n",
        "    H, W = B.shape[:2]\n",
        "    PD = upsample_mask_to(pred_dense_small, (H, W))\n",
        "    PS = upsample_mask_to(pred_spin_small,  (H, W))\n",
        "\n",
        "    BOTH = (0,255,255); SPONLY = (0,255,0); DENSEONLY = (255,0,0)\n",
        "\n",
        "    both = (PD==1) & (PS==1) & (GT==1)\n",
        "    sp   = (PS==1) & (PD==0) & (GT==1)\n",
        "    dn   = (PD==1) & (PS==0) & (GT==1)\n",
        "\n",
        "    color_gt = np.zeros_like(B, dtype=np.uint8)\n",
        "    color_gt[both] = BOTH\n",
        "    color_gt[sp]   = SPONLY\n",
        "    color_gt[dn]   = DENSEONLY\n",
        "\n",
        "    overlay = (B*(1-alpha) + color_gt*alpha).astype(np.uint8)\n",
        "\n",
        "    # —— unmix (for check)：COLOR ≈ clip(2*overlay - B, 0, 255)\n",
        "    color_est = np.clip(2*overlay.astype(np.int16) - B.astype(np.int16), 0, 255).astype(np.uint8)\n",
        "    target_colors = np.array([BOTH, SPONLY, DENSEONLY], dtype=np.uint8)  # 3×3\n",
        "    diff = ((color_est[...,None,:].astype(np.int16) - target_colors.astype(np.int16))**2).sum(axis=-1) # H×W×3\n",
        "    cls = diff.argmin(axis=-1)  # 0/1/2\n",
        "    color_quant = np.zeros_like(B, dtype=np.uint8)\n",
        "    color_quant[GT==1] = target_colors[cls[GT==1]]\n",
        "\n",
        "    overlay_check = (B*(1-alpha) + color_quant*alpha).astype(np.uint8)\n",
        "\n",
        "    return PD, PS, overlay, color_quant, overlay_check\n",
        "\n",
        "# 2×3 check panel\n",
        "def make_check_panel(root: str, name: str,\n",
        "                     pred_dense_small: np.ndarray,\n",
        "                     pred_spin_small:  np.ndarray,\n",
        "                     alpha: float = 0.5) -> Image.Image:\n",
        "    A, B, GT = load_raw_triplet(root, \"test\", name)\n",
        "    _, _, overlay, color_gt, overlay_check = make_overlay_and_colorized(\n",
        "        B, GT, pred_dense_small, pred_spin_small, alpha=alpha\n",
        "    )\n",
        "\n",
        "    def as_gray3(x01: np.ndarray) -> np.ndarray:\n",
        "        g = (x01.astype(np.uint8) * 255)\n",
        "        return np.stack([g,g,g], axis=-1)\n",
        "\n",
        "    row1 = np.concatenate([A, B, as_gray3(GT)], axis=1)\n",
        "    row2 = np.concatenate([overlay, color_gt, overlay_check], axis=1)\n",
        "    panel = np.concatenate([row1, row2], axis=0)\n",
        "\n",
        "    img = Image.fromarray(panel)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    try:\n",
        "        font = ImageFont.load_default()\n",
        "    except Exception:\n",
        "        font = None\n",
        "\n",
        "    H, W = B.shape[0], B.shape[1]\n",
        "    titles = [\n",
        "        (\"A (t1)\", 0,0), (\"B (t2)\", 1,0), (\"Ground Truth\", 2,0),\n",
        "        (\"Overlay (B + color)\", 0,1),\n",
        "        (\"Colorized GT (recovered)\", 1,1),\n",
        "        (\"Re-overlay check\", 2,1),\n",
        "    ]\n",
        "    for text, cx, cy in titles:\n",
        "        x = cx*W + 10; y = cy*H + 10\n",
        "        draw.rectangle([x-6, y-6, x+6+7*len(text), y+18], fill=(255,255,255))\n",
        "        draw.text((x, y), text, fill=(0,0,0), font=font)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Main training script (two methods)\n",
        "# =========================\n",
        "def main():\n",
        "    mount_drive_if_colab()\n",
        "    set_seed(CFG[\"seed\"])\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    root = CFG[\"root\"]\n",
        "    out_dir = CFG[\"out_dir\"]\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Sanity check folders\n",
        "    for sp in (\"train\",\"val\",\"test\"):\n",
        "        for sub in (\"A\",\"B\",\"label\"):\n",
        "            p = os.path.join(root, sp, sub)\n",
        "            if not os.path.isdir(p):\n",
        "                raise FileNotFoundError(f\"Missing folder: {p}\")\n",
        "    print(f\"[OK] Found LEVIR-CD at: {root}\")\n",
        "\n",
        "    # Datasets & loaders\n",
        "    train_set = LevirCD(root, split=\"train\", resize=CFG[\"resize\"], aug=True)\n",
        "    val_set   = LevirCD(root, split=\"val\",   resize=CFG[\"resize\"], aug=False)\n",
        "    test_set  = LevirCD(root, split=\"test\",  resize=CFG[\"resize\"], aug=False)\n",
        "\n",
        "    workers = 2 if torch.cuda.is_available() else 0\n",
        "    pin_mem = True if torch.cuda.is_available() else False\n",
        "    train_loader = DataLoader(train_set, batch_size=CFG[\"batch_size\"], shuffle=True,\n",
        "                              num_workers=workers, pin_memory=pin_mem)\n",
        "    val_loader   = DataLoader(val_set,   batch_size=CFG[\"batch_size\"], shuffle=False,\n",
        "                              num_workers=workers, pin_memory=pin_mem)\n",
        "    test_loader  = DataLoader(test_set,  batch_size=CFG[\"batch_size\"], shuffle=False,\n",
        "                              num_workers=workers, pin_memory=pin_mem)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # ---------- Method A: Dense ----------\n",
        "    dense_dir = os.path.join(out_dir, \"dense\"); os.makedirs(dense_dir, exist_ok=True)\n",
        "    model = FCSiamConc(in_ch=3, feats=(32,64,128,256), use_sp=False).to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=CFG[\"lr\"])\n",
        "    best_f1, best_path = -1.0, os.path.join(dense_dir, \"best.pth\")\n",
        "    print(\"\\n==== Train: DENSE ====\")\n",
        "    for ep in range(1, CFG[\"epochs_per_method\"]+1):\n",
        "        tr = train_one_epoch(model, train_loader, optim, device)\n",
        "        val = evaluate(model, val_loader, device)\n",
        "        print(f\"[Dense] Epoch {ep:02d}/{CFG['epochs_per_method']} | \"\n",
        "              f\"TrainLoss {tr:.4f} | Val F1 {val['mean_F1']:.4f} | Val IoU {val['mean_IoU']:.4f}\")\n",
        "        if val[\"mean_F1\"] > best_f1:\n",
        "            best_f1 = val[\"mean_F1\"]\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "    # Test\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    test_dense = evaluate(model, test_loader, device)\n",
        "    print(\"==> Dense Test:\", {k:round(v,4) for k,v in test_dense.items() if isinstance(v,float)})\n",
        "\n",
        "    results[\"dense\"] = {\"test\": test_dense, \"ckpt\": best_path}\n",
        "\n",
        "    # ---------- Method B: SP-in ----------\n",
        "    spin_dir = os.path.join(out_dir, \"sp_in\"); os.makedirs(spin_dir, exist_ok=True)\n",
        "    model = FCSiamConc(in_ch=3, feats=(32,64,128,256), use_sp=True, sp_kwargs=CFG[\"sp\"]).to(device)\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=CFG[\"lr\"])\n",
        "    best_f1, best_path = -1.0, os.path.join(spin_dir, \"best.pth\")\n",
        "    print(\"\\n==== Train: SP-in ====\")\n",
        "    for ep in range(1, CFG[\"epochs_per_method\"]+1):\n",
        "        tr = train_one_epoch(model, train_loader, optim, device)\n",
        "        val = evaluate(model, val_loader, device)\n",
        "        print(f\"[SP-in] Epoch {ep:02d}/{CFG['epochs_per_method']} | \"\n",
        "              f\"TrainLoss {tr:.4f} | Val F1 {val['mean_F1']:.4f} | Val IoU {val['mean_IoU']:.4f}\")\n",
        "        if val[\"mean_F1\"] > best_f1:\n",
        "            best_f1 = val[\"mean_F1\"]\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "    # Test\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    test_spin = evaluate(model, test_loader, device)\n",
        "    print(\"==> SP-in Test:\", {k:round(v,4) for k,v in test_spin.items() if isinstance(v,float)})\n",
        "\n",
        "    results[\"sp_in\"] = {\"test\": test_spin, \"ckpt\": best_path}\n",
        "\n",
        "    # ---------- Top-10 panels by ΔTP(SP-in − Dense) ----------\n",
        "    dense_preds, spin_preds = test_dense[\"preds\"], test_spin[\"preds\"]   # [N,Hs,Ws]\n",
        "    gts_small, names = test_spin[\"gts\"], test_spin[\"names\"]\n",
        "    assert dense_preds.shape == spin_preds.shape == gts_small.shape\n",
        "\n",
        "    # calculate ΔTP based on TP in GT\n",
        "    deltas = []\n",
        "    for i in range(len(names)):\n",
        "        tp_dense = int(((dense_preds[i]==1) & (gts_small[i]==1)).sum())\n",
        "        tp_spin  = int(((spin_preds[i]==1)  & (gts_small[i]==1)).sum())\n",
        "        deltas.append(tp_spin - tp_dense)\n",
        "\n",
        "    order = np.argsort(np.array(deltas))[::-1]  # from large to small\n",
        "    topk = int(min(10, len(order)))\n",
        "    pick = order[:topk].tolist()\n",
        "    print(f\"[Viz] Selected top-{topk} indices by ΔTP:\", [int(i) for i in pick])\n",
        "\n",
        "    panel_dir = os.path.join(out_dir, \"panels_top10\")\n",
        "    os.makedirs(panel_dir, exist_ok=True)\n",
        "\n",
        "    saved = []\n",
        "    for rank, i in enumerate(pick, 1):\n",
        "        name = names[i]\n",
        "        panel = make_check_panel(\n",
        "            root=CFG[\"root\"],\n",
        "            name=name,\n",
        "            pred_dense_small=dense_preds[i],\n",
        "            pred_spin_small=spin_preds[i],\n",
        "            alpha=0.5,  # same as training overlay\n",
        "        )\n",
        "        fname = f\"panel_rank{rank:02d}_dTP{int(deltas[i])}_{name if name.lower().endswith('.png') else name+'.png'}\"\n",
        "        fpath = os.path.join(panel_dir, fname)\n",
        "        panel.save(fpath)\n",
        "        saved.append(fpath)\n",
        "        print(f\"[Saved] {fpath}\")\n",
        "\n",
        "    # —— save concise summary JSON (avoid numpy object serialization error) ——\n",
        "    summary = {\n",
        "        \"cfg\": CFG,\n",
        "        \"dense\": {\"test_mean_F1\": float(results[\"dense\"][\"test\"][\"mean_F1\"]),\n",
        "                  \"test_mean_IoU\": float(results[\"dense\"][\"test\"][\"mean_IoU\"]),\n",
        "                  \"ckpt\": results[\"dense\"][\"ckpt\"]},\n",
        "        \"sp_in\": {\"test_mean_F1\": float(results[\"sp_in\"][\"test\"][\"mean_F1\"]),\n",
        "                  \"test_mean_IoU\": float(results[\"sp_in\"][\"test\"][\"mean_IoU\"]),\n",
        "                  \"ckpt\": results[\"sp_in\"][\"ckpt\"]},\n",
        "        \"top10_panels\": saved,\n",
        "        \"top10_indices\": [int(i) for i in pick],\n",
        "        \"top10_names\": [str(names[i]) for i in pick],\n",
        "        \"top10_delta_tp\": [int(deltas[i]) for i in pick],\n",
        "    }\n",
        "    with open(os.path.join(out_dir, \"summary.json\"), \"w\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    print(f\"[Saved] summary -> {os.path.join(out_dir, 'summary.json')}\")\n",
        "\n",
        "# =========================\n",
        "# Run\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nBE-9A3pZQwn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBE-9A3pZQwn",
        "outputId": "5d59364d-d604-4e91-c910-0baa735ce41f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_topk_hdlabels/panel_rank01_dTP2609_test_45.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_topk_hdlabels/panel_rank02_dTP1624_test_46.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_topk_hdlabels/panel_rank03_dTP1203_test_80.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_topk_hdlabels/panel_rank04_dTP1132_test_8.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_topk_hdlabels/panel_rank05_dTP790_test_33.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_topk_hdlabels/panel_rank06_dTP758_test_13.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_topk_hdlabels/panel_rank07_dTP693_test_39.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_topk_hdlabels/panel_rank08_dTP624_test_43.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_topk_hdlabels/panel_rank09_dTP613_test_77.png\n",
            "[Saved] /content/drive/MyDrive/LEVIR_CD_runs/panels_topk_hdlabels/panel_rank10_dTP541_test_115.png\n",
            "[Saved] summary -> /content/drive/MyDrive/LEVIR_CD_runs/panels_topk_hdlabels/summary_hdlabels.json\n"
          ]
        }
      ],
      "source": [
        "# ===== Top-K visualization with crisp English labels (no retraining) =====\n",
        "import os, json, numpy as np, torch\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "TOPK = 10            # change to the number you want\n",
        "ALPHA = 0.5          # overlay transparency\n",
        "FONT_SCALE = 0.045   # font size ~ W * 0.045, can be adjusted\n",
        "PAD = 12             # label padding\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def load_raw_triplet(root: str, split: str, name: str):\n",
        "    pa = os.path.join(root, split, \"A\", name)\n",
        "    pb = os.path.join(root, split, \"B\", name)\n",
        "    pg = os.path.join(root, split, \"label\", name)\n",
        "    A = np.array(Image.open(pa).convert(\"RGB\"), dtype=np.uint8)\n",
        "    B = np.array(Image.open(pb).convert(\"RGB\"), dtype=np.uint8)\n",
        "    GT = (np.array(Image.open(pg).convert(\"L\"), dtype=np.uint8) > 127).astype(np.uint8)\n",
        "    return A, B, GT\n",
        "\n",
        "def upsample_mask_to(mask_small: np.ndarray, target_hw):\n",
        "    H, W = target_hw\n",
        "    pil = Image.fromarray(mask_small.astype(np.uint8)*255).resize((W,H), Image.NEAREST)\n",
        "    return (np.array(pil, dtype=np.uint8) > 127).astype(np.uint8)\n",
        "\n",
        "def binarize(logits, thr=0.5):\n",
        "    return (torch.sigmoid(logits) >= thr).to(torch.uint8)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_preds(model, loader, device, thr=0.5):\n",
        "    model.eval()\n",
        "    all_preds, all_gts, all_names = [], [], []\n",
        "    for a,b,y,names in loader:\n",
        "        a,b,y = a.to(device), b.to(device), y.to(device)\n",
        "        pred = binarize(model(a,b), thr=thr).cpu().numpy().astype(np.uint8)[:,0]\n",
        "        gt   = (y.cpu().numpy() > 0.5).astype(np.uint8)[:,0]\n",
        "        all_preds.append(pred); all_gts.append(gt); all_names += list(names)\n",
        "    return np.concatenate(all_preds, 0), np.concatenate(all_gts, 0), all_names\n",
        "\n",
        "def _load_font(px):\n",
        "    # use bold TrueType font; if not available, use default and add bold stroke\n",
        "    for p in [\n",
        "        \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\",\n",
        "        \"/usr/share/fonts/truetype/liberation/LiberationSans-Bold.ttf\",\n",
        "    ]:\n",
        "        if os.path.exists(p):\n",
        "            return ImageFont.truetype(p, px)\n",
        "    return ImageFont.load_default()\n",
        "\n",
        "def draw_label(img: Image.Image, xy, text, box_alpha=180):\n",
        "    \"\"\"clear English label in top-left: bold + white text + semi-transparent white background\"\"\"\n",
        "    draw = ImageDraw.Draw(img, mode=\"RGBA\")\n",
        "    W, H = img.size\n",
        "    # dynamic font size: related to single tile width (here img is the whole panel)\n",
        "    tile_w = W // 3\n",
        "    fs = max(16, int(tile_w * FONT_SCALE))\n",
        "    font = _load_font(fs)\n",
        "\n",
        "    x, y = xy\n",
        "    # text boundary\n",
        "    tw, th = draw.textbbox((0,0), text, font=font)[2:]\n",
        "    bw = tw + PAD*2\n",
        "    bh = th + PAD*2\n",
        "    # semi-transparent white background (rounded corners)\n",
        "    box = Image.new(\"RGBA\", (bw, bh), (255,255,255,box_alpha))\n",
        "    img.paste(box, (x, y), box)\n",
        "    # white text + black stroke (for better clarity)\n",
        "    draw.text((x+PAD, y+PAD), text, font=font, fill=(0,0,0,255), stroke_width=max(2, fs//10), stroke_fill=(255,255,255,255))\n",
        "\n",
        "def make_panel_fixed_hd(root, name, pd_small, ps_small, alpha=0.5) -> Image.Image:\n",
        "    A,B,GT = load_raw_triplet(root, \"test\", name)\n",
        "    H,W = B.shape[:2]\n",
        "    PD = upsample_mask_to(pd_small, (H,W))\n",
        "    PS = upsample_mask_to(ps_small, (H,W))\n",
        "\n",
        "    BOTH = (0,255,255); SPONLY=(0,255,0); DENSEONLY=(255,0,0)\n",
        "    both = (PD==1)&(PS==1)&(GT==1)\n",
        "    sp   = (PS==1)&(PD==0)&(GT==1)\n",
        "    dn   = (PD==1)&(PS==0)&(GT==1)\n",
        "\n",
        "    red_only   = np.zeros_like(B); red_only[dn] = DENSEONLY\n",
        "    green_only = np.zeros_like(B); green_only[sp] = SPONLY\n",
        "    color = np.zeros_like(B); color[both]=BOTH; color[sp]=SPONLY; color[dn]=DENSEONLY\n",
        "    overlay = (B*(1-alpha)+color*alpha).astype(np.uint8)\n",
        "\n",
        "    def gray3(x01): g=(x01.astype(np.uint8)*255); return np.stack([g,g,g],-1)\n",
        "    row1 = np.concatenate([A, B, gray3(GT)], 1)\n",
        "    row2 = np.concatenate([red_only, green_only, overlay], 1)\n",
        "    panel = np.concatenate([row1, row2], 0)\n",
        "    img = Image.fromarray(panel).convert(\"RGBA\")\n",
        "\n",
        "    tile_w, tile_h = W, H\n",
        "    labels = [\n",
        "        (\"A (t1)\", 0,0), (\"B (t2)\", 1,0), (\"Ground Truth\", 2,0),\n",
        "        (\"Dense-only (red, within GT)\", 0,1),\n",
        "        (\"Adding SP-in (green, within GT)\", 1,1),\n",
        "        (\"Overlay (cyan=both, green=SP-in, red=Dense)\", 2,1),\n",
        "    ]\n",
        "    for text, cx, cy in labels:\n",
        "        draw_label(img, (cx*tile_w+10, cy*tile_h+10), text)\n",
        "\n",
        "    return img.convert(\"RGB\")\n",
        "\n",
        "# ---------- build test loader & reload ckpts ----------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "test_set  = LevirCD(CFG[\"root\"], split=\"test\", resize=CFG[\"resize\"], aug=False)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set, batch_size=CFG[\"batch_size\"], shuffle=False,\n",
        "    num_workers=2 if torch.cuda.is_available() else 0, pin_memory=torch.cuda.is_available()\n",
        ")\n",
        "\n",
        "# Dense preds\n",
        "model_dense = FCSiamConc(in_ch=3, feats=(32,64,128,256), use_sp=False).to(device)\n",
        "dense_ckpt = os.path.join(CFG[\"out_dir\"], \"dense\", \"best.pth\")\n",
        "model_dense.load_state_dict(torch.load(dense_ckpt, map_location=device))\n",
        "dense_preds, gts_small, names = eval_preds(model_dense, test_loader, device)\n",
        "\n",
        "# SP-in preds\n",
        "model_spin = FCSiamConc(in_ch=3, feats=(32,64,128,256), use_sp=True, sp_kwargs=CFG[\"sp\"]).to(device)\n",
        "spin_ckpt = os.path.join(CFG[\"out_dir\"], \"sp_in\", \"best.pth\")\n",
        "model_spin.load_state_dict(torch.load(spin_ckpt, map_location=device))\n",
        "spin_preds, _, _ = eval_preds(model_spin, test_loader, device)\n",
        "\n",
        "# ---------- select Top-K by ΔTP ----------\n",
        "deltas = []\n",
        "for i in range(len(names)):\n",
        "    tp_dense = int(((dense_preds[i]==1)&(gts_small[i]==1)).sum())\n",
        "    tp_spin  = int(((spin_preds[i]==1)&(gts_small[i]==1)).sum())\n",
        "    deltas.append(tp_spin - tp_dense)\n",
        "order = np.argsort(np.asarray(deltas))[::-1]\n",
        "pick = order[:min(TOPK, len(order))]\n",
        "\n",
        "# ---------- save ----------\n",
        "panel_dir = os.path.join(CFG[\"out_dir\"], \"panels_topk_hdlabels\")\n",
        "os.makedirs(panel_dir, exist_ok=True)\n",
        "saved = []\n",
        "for rank, idx in enumerate(pick, 1):\n",
        "    name = names[int(idx)]\n",
        "    panel = make_panel_fixed_hd(CFG[\"root\"], name,\n",
        "                                dense_preds[int(idx)], spin_preds[int(idx)], alpha=ALPHA)\n",
        "    fname = f\"panel_rank{rank:02d}_dTP{int(deltas[int(idx)])}_{name if name.lower().endswith('.png') else name+'.png'}\"\n",
        "    fpath = os.path.join(panel_dir, fname)\n",
        "    panel.save(fpath, quality=95)\n",
        "    saved.append(fpath)\n",
        "    print(f\"[Saved] {fpath}\")\n",
        "\n",
        "with open(os.path.join(panel_dir, \"summary_hdlabels.json\"), \"w\") as f:\n",
        "    json.dump({\n",
        "        \"topk\": int(len(pick)),\n",
        "        \"indices\": [int(i) for i in pick],\n",
        "        \"names\": [names[int(i)] for i in pick],\n",
        "        \"delta_tp\": [int(deltas[int(i)]) for i in pick],\n",
        "        \"panels\": saved,\n",
        "        \"dense_ckpt\": dense_ckpt,\n",
        "        \"sp_in_ckpt\": spin_ckpt,\n",
        "        \"alpha\": ALPHA, \"font_scale\": FONT_SCALE\n",
        "    }, f, indent=2)\n",
        "print(f\"[Saved] summary -> {os.path.join(panel_dir, 'summary_hdlabels.json')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hZL9ZI8ZZQoM",
      "metadata": {
        "id": "hZL9ZI8ZZQoM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
